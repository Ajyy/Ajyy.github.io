[{"title":"Neural Machine Translation of Rare Words with Subword Units","url":"%2F2019%2F09%2F15%2FNeural-Machine-Translation-of-Rare-Words-with-Subword-Units%2F","content":"Rico Sennrich and Barry Haddow and Alexandra Birch\nSchool of Informatics, University of Edinburgh\nhttps://arxiv.org/pdf/1508.07909.pdf\n\n## Summary\n\n文章主要是为了解决rare/out-of-vocabulary word的问题，在charator-based和word-based之间取了一个平衡。通过对训练数据进行分析，得出subword unit。因为在ESPNet看到了用subword unit去处理数据，BPE也已经应用在了语音领域，之前交流已经学习过了这个方法，因此再读一遍作为复习。\n\n----------\n\n\n## Problem Statement\n\n解决rare words的翻译问题。 <br />\n\n目前的word-level的方法效果并不算好。论文提到了back-off的方法，即对于rare words，首先翻译成 UNK，然后在look up dictionary去得到结果。但是翻译并不总是一对一的。 <br/>\n\n还有直接把rare words复制到target sentence里，这样的方法或许对姓名之类的单词效果比较好。但存在的问题是翻译常常要求音译或者morphologic change(形态上的变化)，尤其是当单词表不同的时候。\n\n\n\n## Method(s)\n\n**Byte pair encoding(BPE**)\n![](/images_2019.9.15/BPE.png)\n\n*vocab*是一个字典，键值表示每个单词出现的频率，这些单词已经被切割成一个个字符(也可以看成是subword unit)。*num_merges*即生成subword unit的数量，每次循环都生成一个subword unit。在每一个循环中，首先得到*pairs*，*get_stats*会得到所有相邻subword unit的，即任意(subword unit1, subword unit 2)的频率，然后存在*Pairs*里。在得到频率最高的pair即*best*后，用*merge_vocab*在*vocab*里把它们合并起来。\n\n## Evaluation\n\n作者利用BPE和之前提到的word-level的方法，在数据集上测试效果，并且测量了unigram F1和BLEU（略）。\n\n\n\n## Conclusion\n\nBPE在训练数据更少的情况下取得了更好的效果。\n\n## Future Research\n\n研究对于一个翻译任务的最优Vocabulary size。<br />\n\n研究针对双语来说，不仅利用一边的数据去得到Subword Unit，也利用另一边的数据去align。\n","tags":["Machine Translation"]}]